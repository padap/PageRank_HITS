{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Адаменко Павел\n",
    "## Дз: HITS + PageRank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import time\n",
    "import sys\n",
    "import zlib\n",
    "import base64\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import glob\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Директории и файлы для загрузки и выгрузки файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Files innput:\n",
    "FILEURL   = 'lenta/1_10/urls.txt' \n",
    "DOCSFILES = 'lenta/1_10/docs*'\n",
    "\n",
    "#Files output:\n",
    "OUTPUTGRAPHFILE = 'dataNew/1_10/graph1_10.txt'\n",
    "\n",
    "OUTPUTPAGERANK  = 'dataNew/1_10/PageRank1_10.txt'\n",
    "OUTPUTHITS      = 'dataNew/1_10/HITS1_10.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Словари соответствия url:id и id:url \n",
    "\n",
    "(Вопрос: существует ли в питоне биективный словарь?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Build url_id, id_url dicts\n",
    "url_id = {}\n",
    "id_url = {}\n",
    "\n",
    "def parse_single_url(url):\n",
    "    return url[url.find(\"://lenta.ru\")+len(\"://lenta.ru\"):]\n",
    "\n",
    "with open(FILEURL, 'r') as input_file:\n",
    "    for line in input_file:\n",
    "        line_parts = line.rstrip().split('\\t')\n",
    "        doc_id = int(line_parts[0])\n",
    "        url = line_parts[1]\n",
    "        pure_url = parse_single_url(url)\n",
    "        url_id[doc_id] = pure_url\n",
    "        id_url[pure_url] = doc_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Класс построения графа:\n",
    "записывает файл OUTPUTGRAPHFILE в формате:\n",
    "\n",
    "id1 : id2,id3,..idn\n",
    "\n",
    "id2 : ...\n",
    "\n",
    "\n",
    "Где id2, id3,.. idn это ссылки на известные url_id с id1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class make_graph:\n",
    "    \n",
    "    input_path  = ''\n",
    "    output_file = ''\n",
    "    \n",
    "    def __init__(self, input_path, output_file):\n",
    "        self.input_path  = input_path\n",
    "        self.output_file = output_file\n",
    "    \n",
    "    def get_urls_from_html(self, html, output_file):\n",
    "        links = BeautifulSoup(html, 'html').findAll('a')\n",
    "        url_list = []\n",
    "        for link in links:\n",
    "            url = link.get('href')\n",
    "            if url in id_url:\n",
    "                if id_url[url] not in url_list:\n",
    "                    url_list.append(id_url[url])\n",
    "        for c_id, doc_id in enumerate(sorted(url_list)):\n",
    "            if c_id == 0:\n",
    "                output_file.write('%d' % doc_id)\n",
    "            else: \n",
    "                output_file.write(',%d' % doc_id)\n",
    "        return \n",
    "\n",
    "    def build_graph(self):\n",
    "        output_file = self.output_file\n",
    "        input_path  = self.input_path\n",
    "        with open(output_file, 'w') as output_file:\n",
    "            len_file_count = len(glob.glob(input_path))\n",
    "            for file_count, file_name in enumerate(glob.glob(input_path)):\n",
    "                with open(file_name, 'r') as input_file:\n",
    "                    TEXT = input_file.read()\n",
    "                    LEN = TEXT.count('\\n')-1\n",
    "                    for line_id, line in enumerate(TEXT.split('\\n')):\n",
    "                        if '\\t' not in line: continue\n",
    "                        line_parts = line.split('\\t')\n",
    "                        doc_id = int(line_parts[0])\n",
    "                        based64 = line_parts[1] \n",
    "                        sys.stdout.write('\\r: %d / %d doc_id: %d (files %d / %d)' % (line_id, LEN, doc_id, file_count+1, len_file_count))\n",
    "\n",
    "                        html = base64.decodestring(based64).decode('zlib')\n",
    "                        output_file.write('%d : ' % doc_id)\n",
    "                        self.get_urls_from_html(html, output_file)\n",
    "                        output_file.write('\\n')\n",
    "\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Самый долгий кусок кода около 10 часов, на всей выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      ": 0 / 564 doc_id: 0 (files 1 / 1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/padam/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 564 / 564 doc_id: 564 (files 1 / 1)\n",
      "--- 792.896014929 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#speed ~ 1.2 url/sec\n",
    "# 32000*1.2 /60/60 ~ 11 hours\n",
    "start_time = time.time()\n",
    "worker = make_graph(DOCSFILES, OUTPUTGRAPHFILE)\n",
    "\n",
    "####work for 8 hours####\n",
    "worker.build_graph() #\n",
    "########################\n",
    "\n",
    "print(\"\\n--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Класс для работы с полученным графом\n",
    "(описание в коментах)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import random\n",
    "\n",
    "class load_graph:\n",
    "    # загружаем граф из файла, превращаем его в словарь вида:\n",
    "    # {id: {'to':[id1,id2,..],'from':[id1,id2,..]}..}\n",
    "    def __init__(self, input_file_name = OUTPUTGRAPHFILE):\n",
    "        try:\n",
    "            graph = {}\n",
    "            with open(input_file_name, 'r') as input_file:\n",
    "\n",
    "                for line in input_file:\n",
    "\n",
    "                    line_parts = line.rstrip().split(' : ')\n",
    "                    if len(line_parts) < 2: \n",
    "                        continue\n",
    "\n",
    "                    doc_id = int(line_parts[0])\n",
    "                    links_to =  [\n",
    "                        int(linked_id) \n",
    "                        for linked_id in line_parts[1].split(',')#[:-1]\n",
    "                    ]\n",
    "\n",
    "                    if doc_id in graph:\n",
    "                        graph[doc_id]['to'].extend(links_to)\n",
    "                    else:\n",
    "                        graph[doc_id] = {\n",
    "                            'to' : links_to,\n",
    "                            'from' : []\n",
    "                        }\n",
    "\n",
    "                    for linked_id in links_to:\n",
    "                        if linked_id in graph:\n",
    "                            graph[linked_id]['from'].append(doc_id)\n",
    "                        else:\n",
    "                            graph[linked_id] = {\n",
    "                                'to' : [],\n",
    "                                'from' : [doc_id]\n",
    "                            }\n",
    "                    self.graph = graph\n",
    "                    \n",
    "            sys.stdout.write(\" SUCCESS: load graph file: \"+str(input_file_name)+'\\n')\n",
    "        except:\n",
    "            sys.stderr.write(\" FAIL:    Something wrong with load graph: \"+str(input_file_name)+'\\n')\n",
    "    \n",
    "#     def get_graph(self):\n",
    "#         return self.graph\n",
    "    \n",
    "    def _iterationHITS(self, graph):\n",
    "            for doc_id in graph:\n",
    "                auth_to = [ graph[to_id]['author_old']  for to_id in graph[doc_id]['to'] ]\n",
    "                \n",
    "                graph[doc_id]['hubs_new'] = sum(auth_to)\n",
    "\n",
    "                hubs_from = [ graph[from_id]['hubs_old'] for from_id in graph[doc_id]['from']]\n",
    "                graph[doc_id]['author_new'] = sum(hubs_from)\n",
    "            max_auth = math.sqrt(sum([graph[doc_id]['author_new']**2 for doc_id in graph]))\n",
    "            max_hubs = math.sqrt(sum([graph[doc_id]['hubs_new']**2   for doc_id in graph]))\n",
    "            for doc_id in graph:\n",
    "                graph[doc_id]['author_new'] /= 1.0*max_auth\n",
    "                graph[doc_id]['hubs_new']   /= 1.0*max_hubs\n",
    "                graph[doc_id]['author_old']  = graph[doc_id]['author_new']\n",
    "                graph[doc_id]['hubs_old']    = graph[doc_id]['hubs_new']\n",
    "\n",
    "            return graph\n",
    "        \n",
    "    def HITS(self, n_iterations=10):\n",
    "        # algorithm from here https://en.wikipedia.org/wiki/HITS_algorithm\n",
    "        try:\n",
    "            graph = self.graph\n",
    "            self.HITiler  = n_iterations\n",
    "            for doc_id in graph:\n",
    "                graph[doc_id]['author_old'] = 1\n",
    "                graph[doc_id]['author_new'] = 1\n",
    "                graph[doc_id]['hubs_old']   = 1\n",
    "                graph[doc_id]['hubs_new']   = 1\n",
    "\n",
    "            for i in xrange(n_iterations):\n",
    "                sys.stdout.write('\\r: '+str(i+1)+'/'+str(n_iterations)+'...')\n",
    "                graph = self._iterationHITS(graph)\n",
    "            sys.stdout.write('\\r SUCCESS: Calculate HITs ( iter='+str(n_iterations)+')\\n')\n",
    "            self.HITS = graph\n",
    "            return True   \n",
    "        except:\n",
    "            sys.stderr.write('\\r FAIL:    Something wrong with HITs calculation\\n')\n",
    "            return False\n",
    "    \n",
    "    def _iterationPR(self, graph, alpha, N):\n",
    "        for doc_id in graph:\n",
    "            PR_from_sum = sum([(graph[id_from]['old']*1.0 / len(graph[id_from]['to'])) \n",
    "                               for id_from in graph[doc_id]['from']\n",
    "                                ]\n",
    "                            )\n",
    "\n",
    "                        \n",
    "            graph[doc_id]['new'] = alpha*(1.0/N) + (1-alpha)*PR_from_sum\n",
    "            \n",
    "        # dangling nodes teleport \n",
    "        for doc_id in graph:\n",
    "            if graph[doc_id]['from'] == []:\n",
    "                    goList = []\n",
    "                    for go_id in graph:\n",
    "                        if random.random()>alpha:\n",
    "                            goList.append(go_id)\n",
    "                    for go_id in goList:\n",
    "                        graph[go_id]['new'] += (1-alpha)*graph[doc_id]['old']*1.0/len(goList)\n",
    "                        \n",
    "        for doc_id in graph:\n",
    "            graph[doc_id]['old'] = graph[doc_id]['new']\n",
    "#             graph[doc_id]['new'] = 0\n",
    "        \n",
    "        return graph\n",
    "    \n",
    "    def PageRank(self, alpha=0.85, n_iterations=10):\n",
    "        # algorithm from here https://en.wikipedia.org/wiki/PageRank\n",
    "        # dangling nodes treated by random teleport \n",
    "        try:\n",
    "            self.PRalpha = alpha\n",
    "            self.PRiter  = n_iterations\n",
    "            graph = self.graph\n",
    "            N = len(graph)\n",
    "            for doc_id in graph:\n",
    "                graph[doc_id]['old'] = 1.0/N\n",
    "                graph[doc_id]['new'] = 1.0/N  \n",
    "\n",
    "            for i in xrange(n_iterations):\n",
    "                sys.stdout.write('\\r: '+str(i+1)+'/'+str(n_iterations)+'...')\n",
    "                graph = self._iterationPR(graph, alpha, N)\n",
    "            sys.stdout.write('\\r SUCCESS: Calculate Page Rank ( alpha='+str(alpha)+', iter='+str(n_iterations)+' )\\n')\n",
    "            self.PR = graph\n",
    "            return True \n",
    "        except:\n",
    "            sys.stderr.write('\\r FAIL:    Something wrong with PageRank calculation\\n')\n",
    "            return False\n",
    "    \n",
    "    def print_best_by_HITS(self, url_id = url_id, output_file_name=OUTPUTHITS, n_best=30):\n",
    "        try:\n",
    "            graph = self.HITS\n",
    "            \n",
    "            sorted_by_auth = sorted(graph.keys(), key=lambda v: float(graph[v]['author_new']), reverse=True)\n",
    "            sorted_by_hubs = sorted(graph.keys(), key=lambda v: float(graph[v]['hubs_new']),   reverse=True)\n",
    "\n",
    "            with open(output_file_name, 'w') as output_file:\n",
    "\n",
    "                output_file.write('\\tBest HITs authorities ( iter = '+str(self.HITiler)+' ) :\\n\\n')\n",
    "                for doc_id in sorted_by_auth[:n_best]:\n",
    "                    output_file.write(str(doc_id) + '\\thttp://lenta.ru' +str(url_id[doc_id])+'\\t'+str(graph[doc_id]['author_new'])+'\\n')\n",
    "\n",
    "\n",
    "\n",
    "                output_file.write('\\n')\n",
    "                output_file.write('===================================') \n",
    "                output_file.write('\\n\\n\\tBest HITs hubs:\\n\\n')\n",
    "                \n",
    "                for doc_id in sorted_by_hubs[:n_best]:\n",
    "                    output_file.write(str(doc_id) + '\\thttp://lenta.ru' +str(url_id[doc_id])+'\\t'+str(graph[doc_id]['hubs_new'])+'\\n')\n",
    "                    \n",
    "            sys.stdout.write(\" SUCCESS: Output for HITS in \"+str(output_file_name)+'\\n')\n",
    "            return True\n",
    "        except:\n",
    "            sys.stderr.write(\" FAIL:    Something wrong with HITS output :(\\n\")\n",
    "            return False\n",
    "        \n",
    "    def print_best_by_PageRank(self, url_id = url_id, output_file_name=OUTPUTPAGERANK, n_best=30):\n",
    "        try:\n",
    "            graph = self.PR\n",
    "            sorted_by_PR = sorted(graph.keys(), key=lambda v: float(graph[v]['new']), reverse=True)\n",
    "            \n",
    "            with open(output_file_name, 'w') as output_file:\n",
    "                output_file.write('\\tBest PageRank authorities ( iter='+str(self.PRiter)+', alpha='+str(self.PRalpha)+' ) :\\n\\n')\n",
    "                for doc_id in sorted_by_PR[:n_best]:\n",
    "                    output_file.write(str(doc_id) + '\\thttp://lenta.ru' +str(url_id[doc_id])+'\\t'+str(graph[doc_id]['new'])+'\\n')\n",
    "                    \n",
    "            sys.stdout.write(\" SUCCESS: Output for PageRank in \"+str(output_file_name)+'\\n')\n",
    "            return True\n",
    "        except:\n",
    "            sys.stderr.write(\" FAIL:    Something wrong with PageRank output :(\\n\")\n",
    "            return False\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SUCCESS: load graph file: dataNew/1_10/graph1_10.txt\n",
      " SUCCESS: Calculate HITs ( iter=10)\n",
      ": 1/10..."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    worker1 = load_graph()\n",
    "#     print(worker1.graph)\n",
    "#     graph = worker.get_graph()\n",
    "    HITS = worker1.HITS()\n",
    "    PR   = worker1.PageRank()\n",
    "    worker1.print_best_by_HITS()\n",
    "    worker1.print_best_by_PageRank()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
